{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git\n!pip install transformers","metadata":{"id":"eSIib9XGY6Fs","execution":{"iopub.status.busy":"2023-03-02T01:16:08.668400Z","iopub.execute_input":"2023-03-02T01:16:08.668716Z","iopub.status.idle":"2023-03-02T01:16:33.459831Z","shell.execute_reply.started":"2023-03-02T01:16:08.668685Z","shell.execute_reply":"2023-03-02T01:16:33.458519Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-vzn_uoe8\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-vzn_uoe8\n  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m601.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (2021.11.10)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (4.64.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (1.13.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (0.14.0)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->clip==1.0) (0.2.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->clip==1.0) (4.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (2.28.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->clip==1.0) (9.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->clip==1.0) (2.1.1)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=524bb1e5675af9d2600c3d408d990097863a18bac9101adf8ba70884aa76681a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_yg_th6a/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install more_itertools","metadata":{"id":"yHciHyC2bDYS","execution":{"iopub.status.busy":"2023-03-02T01:16:33.462605Z","iopub.execute_input":"2023-03-02T01:16:33.463042Z","iopub.status.idle":"2023-03-02T01:16:43.770871Z","shell.execute_reply.started":"2023-03-02T01:16:33.462985Z","shell.execute_reply":"2023-03-02T01:16:43.769638Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting more_itertools\n  Downloading more_itertools-9.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m654.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: more_itertools\nSuccessfully installed more_itertools-9.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"Mlh07tT9fuc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip aa.zip","metadata":{"id":"8m4D94FWcTq6","execution":{"iopub.status.busy":"2023-03-01T11:45:21.823637Z","iopub.execute_input":"2023-03-01T11:45:21.824241Z","iopub.status.idle":"2023-03-01T11:45:23.003128Z","shell.execute_reply.started":"2023-03-01T11:45:21.824173Z","shell.execute_reply":"2023-03-01T11:45:23.001373Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"unzip:  cannot find or open aa.zip, aa.zip.zip or aa.zip.ZIP.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb\n!wandb login","metadata":{"id":"u0HEiMnKroae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import clip\nimport os\nfrom torch import nn\nimport numpy as np\nimport torch\nimport torch.nn.functional as nnf\nimport sys\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom tqdm import tqdm, trange\nimport PIL.Image\nfrom IPython.display import Image \nimport pandas as pd\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n","metadata":{"id":"jAGDCFZ6aOCA","execution":{"iopub.status.busy":"2023-03-02T01:16:47.232799Z","iopub.execute_input":"2023-03-02T01:16:47.233909Z","iopub.status.idle":"2023-03-02T01:16:58.210669Z","shell.execute_reply.started":"2023-03-02T01:16:47.233841Z","shell.execute_reply":"2023-03-02T01:16:58.209383Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\n\nclass MLP(nn.Module):\n    \n    def __init__(self, sizes, bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) -1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n    \n\n\nclass ClipCaptionModel(nn.Module):\n    \n    def __init__(self, prefix_length, prefix_size: int = 768):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n\n        self.gpt = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')\n        \n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        \n        if prefix_length > 10:  # not enough memory\n            self.clip_project = nn.Linear(10,47)#prefix_size, self.gpt_embedding_size * prefix_length)\n        else:\n            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n\n    #@functools.lru_cache #FIXME\n    def get_dummy_token(self, batch_size, device):\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n\n    def forward(self, tokens, prefix, mask, labels):\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n    \n\nclass ClipCaptionPrefix(ClipCaptionModel):\n\n    def parameters(self, recurse = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self","metadata":{"id":"85FkKUgCaQLz","execution":{"iopub.status.busy":"2023-03-02T01:16:58.213542Z","iopub.execute_input":"2023-03-02T01:16:58.214340Z","iopub.status.idle":"2023-03-02T01:16:58.230143Z","shell.execute_reply.started":"2023-03-02T01:16:58.214294Z","shell.execute_reply":"2023-03-02T01:16:58.229097Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"1BGtgf3oaSRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nto-base-csv/new_train.csv')\ndf_test = pd.read_csv('/kaggle/input/nto-base-csv/new_test.csv')","metadata":{"id":"S21C4pkyarFN","execution":{"iopub.status.busy":"2023-03-01T13:27:34.591579Z","iopub.execute_input":"2023-03-01T13:27:34.591977Z","iopub.status.idle":"2023-03-01T13:27:34.730793Z","shell.execute_reply.started":"2023-03-01T13:27:34.591937Z","shell.execute_reply":"2023-03-01T13:27:34.729771Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"new_all = pd.concat([df_train, df_test])","metadata":{"execution":{"iopub.status.busy":"2023-03-01T13:28:40.359050Z","iopub.execute_input":"2023-03-01T13:28:40.359631Z","iopub.status.idle":"2023-03-01T13:28:40.369037Z","shell.execute_reply.started":"2023-03-01T13:28:40.359596Z","shell.execute_reply":"2023-03-01T13:28:40.368049Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\ndata = []\nfor video_name, question, answer in zip(df_train.video_name, df_train.question, df_train.answer):\n    name = f'videos/{video_name}.mp4'\n    if os.path.exists(name):  \n        data += [(name,f'Q: {question} A: {answer}')]","metadata":{"id":"nRjeRPvia6uv"},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import io\nimport os\nimport PIL\nimport random\nimport numpy as np\nimport torch\nimport torchvision\nimport transformers\nimport more_itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nfrom dataclasses import dataclass, field\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nimport cv2\nimport clip\nfrom PIL import Image\nimport pickle\nfrom tqdm.contrib import tzip\nfrom tqdm.notebook import tqdm\n\ndef image_grid(imgs, rows, cols):\n    pils = imgs\n    \n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n    \n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\ndef read_video(path, transform=None, frames_num=9, window=30):\n    frames = []\n    cap = cv2.VideoCapture(path)\n    \n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    N = length//(frames_num)\n    #print(length)\n    #counter = \n    \n    current_frame = 1\n    for i in range(length):\n    \n        ret, frame = cap.read(current_frame)\n        \n        if ret and i==current_frame and len(frames)<frames_num:\n            size = 112, 112\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame = cv2.resize(frame, (112, 112))\n            frame = Image.fromarray(frame)\n#             frame.thumbnail(size, Image.ANTIALIAS)\n            \n            frames.append(frame)\n            current_frame += N\n        \n       \n        #print(current_frame)\n        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n        \n        \n    cap.release()\n    #print(frames)\n    return frames","metadata":{"id":"Bjj0uB4gccCS","execution":{"iopub.status.busy":"2023-03-02T01:16:58.232561Z","iopub.execute_input":"2023-03-02T01:16:58.233005Z","iopub.status.idle":"2023-03-02T01:16:58.413072Z","shell.execute_reply.started":"2023-03-02T01:16:58.232950Z","shell.execute_reply":"2023-03-02T01:16:58.411996Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda')\nclip_model_type = \"ViT-L/14@336px\"\n\nout_path_train = f\"Features_train_full.pkl\"\nout_path_test = f\"Features_train_full_test.pkl\"\nvideo_path =  '/kaggle/input/nto-base/videos'\n\n\nclip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n\n# path_a = 'activitynet-qa/dataset/train_a.json'\n# path_q = 'activitynet-qa/dataset/train_q.json'\n# df_a = pd.read_json(path_a)\n# df_q = pd.read_json(path_q)","metadata":{"id":"0uR5W2TTccdv","execution":{"iopub.status.busy":"2023-03-01T13:29:06.012997Z","iopub.execute_input":"2023-03-01T13:29:06.013832Z","iopub.status.idle":"2023-03-01T13:29:46.008577Z","shell.execute_reply.started":"2023-03-01T13:29:06.013793Z","shell.execute_reply":"2023-03-01T13:29:46.007461Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 891M/891M [00:22<00:00, 40.9MiB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"clip_model.to(device)\nNone","metadata":{"id":"0fBnjfERcjlp","execution":{"iopub.status.busy":"2023-03-01T13:29:46.010752Z","iopub.execute_input":"2023-03-01T13:29:46.011362Z","iopub.status.idle":"2023-03-01T13:29:46.023535Z","shell.execute_reply.started":"2023-03-01T13:29:46.011322Z","shell.execute_reply":"2023-03-01T13:29:46.022507Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"all_embeddings = []\nall_captions = []\ni = 0\n\nfor video_name, question, answer in tzip(new_all.video_name, new_all.question, new_all.answer):\n    \n    \n    name = f'{video_path}/{video_name}.mp4'\n    \n    text = f'Q: {question} A: {answer}'\n    #print(name)\n    if os.path.exists(name):\n        \n        video = read_video(path = name, frames_num=9)\n        if len(video)>1:\n            #print(len(video))\n            image = image_grid(video,3,3)\n\n            image = preprocess(image).unsqueeze(0).to(device)\n            with torch.no_grad():\n                prefix = clip_model.encode_image(image).cpu()\n            #d[\"clip_embedding\"] = i\n            all_embeddings.append(prefix)\n            all_captions.append(text)\n    \nwith open(out_path_train, 'wb') as f:\n    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n\nprint('Done')\nprint(\"%0d embeddings saved \" % len(all_embeddings))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71,"referenced_widgets":["f84af7dfce4a4153a907e52b1a0ff3fe"]},"id":"kwOK5wascju4","outputId":"809def4a-2b3c-45ed-b429-cb242219bc5d","execution":{"iopub.status.busy":"2023-03-01T13:29:58.682069Z","iopub.execute_input":"2023-03-01T13:29:58.682745Z","iopub.status.idle":"2023-03-01T14:54:49.451247Z","shell.execute_reply.started":"2023-03-01T13:29:58.682707Z","shell.execute_reply":"2023-03-01T14:54:49.450028Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/21830 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f473f3bf675b48e697a1ee16cec07ae3"}},"metadata":{}},{"name":"stdout","text":"Done\n21830 embeddings saved \n","output_type":"stream"}]},{"cell_type":"code","source":"'dsfdsfdsf'","metadata":{"execution":{"iopub.status.busy":"2023-03-01T14:54:49.874897Z","iopub.execute_input":"2023-03-01T14:54:49.875631Z","iopub.status.idle":"2023-03-01T14:54:49.882473Z","shell.execute_reply.started":"2023-03-01T14:54:49.875592Z","shell.execute_reply":"2023-03-01T14:54:49.881175Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'dsfdsfdsf'"},"metadata":{}}]},{"cell_type":"code","source":"all_embeddings = []\nall_captions = []\ni = 0\n\nfor video_name, question, answer in tzip(df_test.video_name, df_test.question, df_test.answer):\n    \n    \n    name = f'{video_path}/{video_name}.mp4'\n    \n    text = f'Q: {question} A: {answer}'\n    #print(name)\n    if os.path.exists(name):\n        \n        video = read_video(path = name, frames_num=9)\n        if len(video)>1:\n            #print(len(video))\n            image = image_grid(video,3,3)\n\n            image = preprocess(image).unsqueeze(0).to(device)\n            with torch.no_grad():\n                prefix = clip_model.encode_image(image).cpu()\n            #d[\"clip_embedding\"] = i\n            all_embeddings.append(prefix)\n            all_captions.append(text)\n    \nwith open(out_path_test, 'wb') as f:\n    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n\nprint('Done')\nprint(\"%0d embeddings saved \" % len(all_embeddings))","metadata":{"execution":{"iopub.status.busy":"2023-03-01T08:16:19.487773Z","iopub.execute_input":"2023-03-01T08:16:19.488492Z","iopub.status.idle":"2023-03-01T08:25:15.601182Z","shell.execute_reply.started":"2023-03-01T08:16:19.488456Z","shell.execute_reply":"2023-03-01T08:25:15.599848Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8356a9496c4a41ac8ab2839ec3ebe4c2"}},"metadata":{}},{"name":"stdout","text":"Done\n2183 embeddings saved \n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport gc\nimport io\n\n\nimport random\nimport numpy as np\n\nimport torchvision\nimport transformers\nimport more_itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nfrom dataclasses import dataclass, field\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nimport cv2\nfrom PIL import Image\nimport pickle\nimport sys\nfrom tqdm.contrib import tzip\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as nnf\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n\nimport os\nimport pickle\nimport sys\nimport argparse\nimport json\nfrom typing import Tuple, Optional, Union\nfrom torch.cuda.amp import autocast\n\nfrom transformers.optimization import Adafactor, AdafactorSchedule\nimport wandb\nimport torch\n\nfrom torch.utils.checkpoint import checkpoint_sequential\n\nclass ClipCocoDataset(Dataset):\n    \n    def __init__(self, data_path: str,  prefix_length= 50, gpt2_type = \"sberbank-ai/rugpt3large_based_on_gpt2\",\n                 normalize_prefix=False):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.prefix_length = prefix_length\n        self.normalize_prefix = normalize_prefix\n        with open(data_path, 'rb') as f:\n            all_data = pickle.load(f)\n        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n        sys.stdout.flush()\n        self.prefixes = all_data[\"clip_embedding\"]\n        captions_raw = all_data[\"captions\"]\n        \n        #self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n        \n        self.captions = captions_raw\n        \n        \n        self.captions_tokens = []\n        self.caption2embedding = []\n        max_seq_len = 0\n        i=0\n        for caption in tqdm(captions_raw):\n                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))\n                self.caption2embedding.append(self.prefixes[i])\n                i+=1\n                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n            # self.max_seq_len = max_seq_len\n        #del self.captions_tokens\n        #del self.caption2embedding\n        #gc.collect()\n        #with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n        #        pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n       \n    \n    \n        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n\n    def pad_tokens(self, item: int):\n        tokens = self.captions_tokens[item]\n        padding = self.max_seq_len - tokens.shape[0]\n        if padding > 0:\n            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n            self.captions_tokens[item] = tokens\n        elif padding < 0:\n            tokens = tokens[:self.max_seq_len]\n            self.captions_tokens[item] = tokens\n        mask = tokens.ge(0)  # mask is zero where we out of sequence\n        tokens[~mask] = 0\n        mask = mask.float()\n        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n        return tokens, mask\n    \n    def __len__(self) -> int:\n        return len(self.captions_tokens)\n\n   \n\n    def __getitem__(self, item):\n        tokens, mask = self.pad_tokens(item)\n        prefix = self.prefixes[item]\n        if self.normalize_prefix:\n            prefix = prefix.float()\n            prefix = prefix / prefix.norm(2, -1)\n        return tokens, mask, prefix\n\n    ","metadata":{"id":"5uF07hEqcltu","execution":{"iopub.status.busy":"2023-03-02T01:16:58.415119Z","iopub.execute_input":"2023-03-02T01:16:58.415407Z","iopub.status.idle":"2023-03-02T01:16:58.442126Z","shell.execute_reply.started":"2023-03-02T01:16:58.415380Z","shell.execute_reply":"2023-03-02T01:16:58.441186Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = ClipCocoDataset('Features_train_full_ru.pkl', prefix_length=50, normalize_prefix=False)","metadata":{"id":"H6d-G11Bc-Lb"},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"Data size is 27570\n"},{"name":"stderr","output_type":"stream","text":"100%|██████████| 27570/27570 [00:03<00:00, 7264.12it/s]\n"}]},{"cell_type":"code","source":"! e461a6a3bca9f7cec3390a40dc10cdf576ce3252","metadata":{"id":"51d4XF65f7Eh"},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n\nAborted!\n"}]},{"cell_type":"code","source":"\n# wandb.init(project=\"clip_caption_video\")\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n        super(MLP, self).__init__()\n        layers = []\n        for i in range(len(sizes) - 1):\n            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n            if i < len(sizes) - 2:\n                layers.append(act())\n        self.model = nn.Sequential(*layers)\n    \n    #@autocast()  \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.model(x)\n\n    \ndef freeze(\n    model,\n    freeze_emb=False,\n    freeze_ln=False,\n    freeze_attn=True,\n    freeze_ff=True,\n    freeze_other=True,\n):\n    \n    for name, p in model.named_parameters():\n    # freeze all parameters except the layernorm and positional embeddings\n       \n       \n        \n        name = name.lower()\n        if 'ln' in name or 'norm' in name:\n            p.requires_grad = not freeze_ln\n        elif 'embeddings' in name:\n            p.requires_grad = not freeze_emb\n        elif 'mlp' in name:\n            p.requires_grad = not freeze_ff\n        elif 'attn' in name:\n            p.requires_grad = not freeze_attn\n        else:\n            p.requires_grad = not freeze_other\n           \n    return model\n\nclass ClipCaptionModel(nn.Module):\n    def __init__(self, backbone, prefix_length: int, prefix_size: int = 768):\n        super(ClipCaptionModel, self).__init__()\n        self.prefix_length = prefix_length\n        \"\"\"\n        ru gpts shit\n        \n        \"\"\"\n        self.gpt = GPT2LMHeadModel.from_pretrained(backbone)\n        #self.gpt = freeze(self.gpt)\n        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n        self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n                                  self.gpt_embedding_size * prefix_length))\n\n    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n    \n    # @autocast() \n    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n                labels: Optional[torch.Tensor] = None):\n\n        embedding_text = self.gpt.transformer.wte(tokens)\n        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n\n        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n        if labels is not None:\n            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n            labels = torch.cat((dummy_token, tokens), dim=1)\n        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n        return out\n\n\nclass ClipCaptionPrefix(ClipCaptionModel):\n\n    def parameters(self, recurse: bool = True):\n        return self.clip_project.parameters()\n\n    def train(self, mode: bool = True):\n        super(ClipCaptionPrefix, self).train(mode)\n        self.gpt.eval()\n        return self\n\n\n\n\ndef train(dataset, model: ClipCaptionModel, args,\n          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n\n    device = torch.device('cuda')# xm.xla_device()\n    #\n    batch_size = args.bs\n    epochs = args.epochs\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    model = model.to(device)\n    \n    model = freeze(model)\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=args.lr,betas=(0.9, 0.995))\n    #optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n    #optimizer = SM3(model.parameters(),lr=args.lr)\n    #Adafactor(model.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n\n    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n#     test_dataloader = DataLoader(dataset_test, batch_size=1, shuffle=True, drop_last=True)\n    \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n    )\n    #AdafactorSchedule(optimizer)#num_training_steps=epochs * len(train_dataloader\n    #save_config(args)\n    #print\n\n    for epoch in range(epochs):\n        train_loss = []\n        print(f\">>> Training epoch {epoch}\")\n        sys.stdout.flush()\n        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n        step=0\n        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n            model.zero_grad()\n            step+=1\n            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n            \n            outputs = model(tokens, prefix, mask)\n            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n\n            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n\n            segments = 2\n\n           \n            #out = checkpoint_sequential(modules, segments, input_var)\n\n            # backpropagate\n            loss.backward()\n            #optimizer.zero_grad()\n            progress.set_postfix({\"loss\": loss.item()})\n            train_loss += [loss.item()]\n            clipping_value = 0.5 # arbitrary value of your choosing\n            #torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n            optimizer.step()\n            scheduler.step()\n            \n#             wandb.log({\"loss\":  loss.item()})\n            \n            progress.update()\n            \n\n            del tokens\n            del mask\n            del prefix\n            torch.clear_autocast_cache()\n            torch.cuda.empty_cache()\n            \n            if (idx + 1) % 7000 == 0:\n                torch.save(\n                    model.state_dict(),\n                    \n                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n                )\n#         test_loss = []\n#         model.eval()\n#         for idx, (tokens, mask, prefix) in enumerate(test_dataloader):\n#             tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n#             with torch.no_grad():\n#                 outputs = model(tokens, prefix, mask)\n#             logits = outputs.logits[:, dataset_test.prefix_length - 1: -1]\n\n#             loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n#             test_loss.append(loss)\n            \n#         print('test_loss:', sum(test_loss) / len(test_loss))\n        print('train_loss:', sum(train_loss) / len(train_loss))\n        progress.close()\n        if epoch % args.save_every ==0:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n            )\n    return model\n\n\n\nclass Args():\n    def __init__(self):\n        self.backbone = 'sberbank-ai/rugpt3small_based_on_gpt2'\n        self.data = 'Features_train_full_ru.pkl'\n        self.out_dir = 'checkpoints_larger'\n        self.prefix = 'prefix_1'\n        self.epochs = 21\n        self.save_every = 4\n        self.prefix_length = 50\n        self.bs = 32\n        self.only_prefix = False\n        self.lr = 5e-4\n        \n\n\ndef main():\n\n    \n    args = Args()\n#     wandb.config = {\n#       \"learning_rate\": args.lr,\n#       \"epochs\": args.epochs,\n#       \"batch_size\": args.bs\n#     }\n\n    prefix_length = args.prefix_length\n\n#     dataset = ClipCocoDataset(args.data, prefix_length)\n    dataset = ClipCocoDataset('/kaggle/input/new-embeds/Features_train_full.pkl', prefix_length)\n#     dataset_test = ClipCocoDataset('/kaggle/input/nto-embeds/Features_train_full_test.pkl', prefix_length)\n    \n   \n    #model_path = 'prefix_1-003.pt'\n    model = ClipCaptionModel(backbone = 'sberbank-ai/rugpt3small_based_on_gpt2', prefix_length = 50)\n    # model.load_state_dict(torch.load(model_path, map_location='cpu')) \n    print(\"Train both prefix and GPT\")\n    sys.stdout.flush()\n    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"id":"K3N3MQ7afj8m","outputId":"986bd87f-084a-470f-e25f-953e760b389d","execution":{"iopub.status.busy":"2023-03-02T01:17:52.058882Z","iopub.execute_input":"2023-03-02T01:17:52.059250Z","iopub.status.idle":"2023-03-02T01:17:52.089383Z","shell.execute_reply.started":"2023-03-02T01:17:52.059216Z","shell.execute_reply":"2023-03-02T01:17:52.088202Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset = ClipCocoDataset('/kaggle/input/new-embeds/Features_train_full.pkl', prefix_length=50)\n# dataset_test = ClipCocoDataset('/kaggle/input/nto-embeds/Features_train_full_test.pkl', prefix_length=50)","metadata":{"id":"TqnMOi63Hkbs","execution":{"iopub.status.busy":"2023-03-02T01:17:03.664194Z","iopub.execute_input":"2023-03-02T01:17:03.664939Z","iopub.status.idle":"2023-03-02T01:17:15.499236Z","shell.execute_reply.started":"2023-03-02T01:17:03.664901Z","shell.execute_reply":"2023-03-02T01:17:15.498169Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b892fbfbdf714cf6b169e178555dbd24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf19e6ca2ce94caabde1d6fea96ecb81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b9e913afd7e44709213a43aea130aea"}},"metadata":{}},{"name":"stdout","text":"Data size is 21830\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 21830/21830 [00:06<00:00, 3148.46it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -r checkpoints*","metadata":{"id":"R9cI3gxHhDY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qT6UdHwPhJGa","outputId":"da69f7af-2734-47a8-9c97-67e69e6b2fd9","execution":{"iopub.status.busy":"2023-03-02T01:17:55.791611Z","iopub.execute_input":"2023-03-02T01:17:55.792352Z","iopub.status.idle":"2023-03-02T03:02:11.740085Z","shell.execute_reply.started":"2023-03-02T01:17:55.792311Z","shell.execute_reply":"2023-03-02T03:02:11.737392Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Data size is 21830\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 21830/21830 [00:06<00:00, 3407.27it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08593c7457a04a6e9ca883d10c08bee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/551M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72ab7f1647694be98e9228f2c5967e37"}},"metadata":{}},{"name":"stdout","text":"Train both prefix and GPT\n>>> Training epoch 0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\nprefix_1: 100%|██████████| 682/682 [04:54<00:00,  2.31it/s, loss=3.67]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 5.513370466721722\n>>> Training epoch 1\n","output_type":"stream"},{"name":"stderr","text":"prefix_1: 100%|██████████| 682/682 [04:53<00:00,  2.33it/s, loss=2.41]","output_type":"stream"},{"name":"stdout","text":"train_loss: 2.8178890527518266\n>>> Training epoch 2\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2.35]","output_type":"stream"},{"name":"stdout","text":"train_loss: 2.3289091017239256\n>>> Training epoch 3\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.89]","output_type":"stream"},{"name":"stdout","text":"train_loss: 2.1752994656562805\n>>> Training epoch 4\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2.04]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 2.0960518838373448\n>>> Training epoch 5\n","output_type":"stream"},{"name":"stderr","text":"prefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2.27]","output_type":"stream"},{"name":"stdout","text":"train_loss: 2.0456448454311515\n>>> Training epoch 6\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.93]","output_type":"stream"},{"name":"stdout","text":"train_loss: 2.0085148323665964\n>>> Training epoch 7\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2.2] ","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.9775085742872132\n>>> Training epoch 8\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2.08]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.9545600903698426\n>>> Training epoch 9\n","output_type":"stream"},{"name":"stderr","text":"prefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2.17]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.9360838100707427\n>>> Training epoch 10\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.92]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.9217952991510765\n>>> Training epoch 11\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2.13]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.9107541282855172\n>>> Training epoch 12\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:53<00:00,  2.33it/s, loss=2.12]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.9000166051548597\n>>> Training epoch 13\n","output_type":"stream"},{"name":"stderr","text":"prefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.75]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.8917968474175573\n>>> Training epoch 14\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.94]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.8848301770050855\n>>> Training epoch 15\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.86]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.879729218846542\n>>> Training epoch 16\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2.07]\n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.8755450638508167\n>>> Training epoch 17\n","output_type":"stream"},{"name":"stderr","text":"prefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.92]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.8731079192455213\n>>> Training epoch 18\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.77]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.8701453997242836\n>>> Training epoch 19\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=1.88]","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.8667258081897613\n>>> Training epoch 20\n","output_type":"stream"},{"name":"stderr","text":"\nprefix_1: 100%|██████████| 682/682 [04:52<00:00,  2.33it/s, loss=2]   \n","output_type":"stream"},{"name":"stdout","text":"train_loss: 1.8650058694948548\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:445] . PytorchStreamWriter failed writing file data/174: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/12804068.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train both prefix and GPT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/12804068.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, args, warmup_steps, output_dir, output_prefix)\u001b[0m\n\u001b[1;32m    185\u001b[0m             torch.save(\n\u001b[1;32m    186\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{output_prefix}-{epoch:03d}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             )\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:325] . unexpected pos 401441536 vs 401441424"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:325] . unexpected pos 401441536 vs 401441424","output_type":"error"}]},{"cell_type":"code","source":"'dsf'","metadata":{"execution":{"iopub.status.busy":"2023-03-02T03:02:20.579540Z","iopub.execute_input":"2023-03-02T03:02:20.580304Z","iopub.status.idle":"2023-03-02T03:02:20.587298Z","shell.execute_reply.started":"2023-03-02T03:02:20.580262Z","shell.execute_reply":"2023-03-02T03:02:20.586220Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'dsf'"},"metadata":{}}]},{"cell_type":"code","source":"'/kaggle/working/checkpoints_larger/prefix_1-020.pt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\n!ls\nfrom IPython.display import FileLink\n\nFileLink(r'checkpoints_larger/prefix_1-020.pt')","metadata":{"execution":{"iopub.status.busy":"2023-03-02T03:05:22.375095Z","iopub.execute_input":"2023-03-02T03:05:22.376242Z","iopub.status.idle":"2023-03-02T03:05:23.559092Z","shell.execute_reply.started":"2023-03-02T03:05:22.376180Z","shell.execute_reply":"2023-03-02T03:05:23.557806Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"__notebook_source__.ipynb  checkpoints_larger\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoints_larger/prefix_1-020.pt","text/html":"<a href='checkpoints_larger/prefix_1-020.pt' target='_blank'>checkpoints_larger/prefix_1-020.pt</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"''","metadata":{"execution":{"iopub.status.busy":"2023-03-02T01:04:15.367177Z","iopub.execute_input":"2023-03-02T01:04:15.367545Z","iopub.status.idle":"2023-03-02T01:04:15.398024Z","shell.execute_reply.started":"2023-03-02T01:04:15.367488Z","shell.execute_reply":"2023-03-02T01:04:15.396943Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"''"},"metadata":{}}]},{"cell_type":"code","source":"'sdfdf'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /content/checkpoints/prefix_1-007.pt /content/drive/MyDrive/4","metadata":{"id":"NyO_SyvJAepw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /content/drive/MyDrive/4/prefix_1-007.pt .","metadata":{"id":"9_hej1zYCopu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tg-logger","metadata":{"id":"doKt6VnHAMc5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import clip","metadata":{"id":"FxwaqhsMHmM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\nclip_model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, jit=False)\ntokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')\nprefix_length= 50\nmodel_path = 'prefix_1-007.pt'\nmodel = ClipCaptionModel(backbone = 'gpt2', prefix_length = 50)\nmodel.load_state_dict(torch.load(model_path, map_location='cpu')) \nmodel.to(device)\nNone","metadata":{"id":"sqNobiKVGTVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"VnGs2uMtH3IW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import io\nimport os\nimport PIL\nimport random\nimport numpy as np\nimport torch\nimport torchvision\nimport transformers\nimport more_itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from tqdm import tqdm\nimport pandas as pd\nfrom torch.utils.data import Dataset\n#from tqdm import tqdm\nfrom dataclasses import dataclass, field\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nimport cv2\nfrom PIL import Image\ndef image_grid(imgs, rows, cols):\n    pils = imgs\n    \n    assert len(imgs) == rows*cols\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    grid_w, grid_h = grid.size\n    \n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\ndef read_video(path, transform=None, frames_num=9, window=30):\n    frames = []\n    cap = cv2.VideoCapture(path)\n    \n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    N = length//(frames_num)\n    #print(length)\n    #counter = \n    \n    current_frame = 1\n    for i in range(length):\n    \n        #frameId = int(round(cap.get(current_frame))) \n        #print(current_frame)\n        ret, frame = cap.read(current_frame)\n    \n        \n        \n        \n        \n        if ret and i==current_frame and len(frames)<frames_num:\n            size = 193, 193\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            frame.thumbnail(size, Image.ANTIALIAS)\n            \n            frames.append(frame)\n            current_frame += N\n        \n       \n        #print(current_frame)\n        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n        \n        \n    cap.release()\n    #print(frames)\n    return frames\n\n\n\ndef filter_ngrams(output_text):\n    a_pos = output_text.find(' A:')\n    sec_a_pos = output_text.find(' A:', a_pos + 1)\n    \n    return output_text[:sec_a_pos]\n\ndef generate2(\n        model,\n        tokenizer,\n        tokens=None,\n        prompt='',\n        embed=None,\n        entry_count=1,\n        entry_length=67,  # maximum number of words\n        top_p=0.98,\n        temperature=1.,\n        stop_token = '.',\n):\n    model.eval()\n    generated_num = 0\n    generated_list = []\n    stop_token_index = tokenizer.encode(stop_token)[0]\n    filter_value = -float(\"Inf\")\n    device = next(model.parameters()).device\n\n    with torch.no_grad():\n\n        for entry_idx in range(entry_count):\n            if not tokens:\n                    tokens = torch.tensor(tokenizer.encode(prompt))\n                    #print('tokens',tokens)\n                    tokens = tokens.unsqueeze(0).to(device)\n                    \n            emb_tokens = model.gpt.transformer.wte(tokens)\n            \n            if embed is not None:\n                generated = torch.cat((embed, emb_tokens), dim=1)\n            else:\n                generated = emb_tokens\n\n            for i in range(entry_length):\n\n                outputs = model.gpt(inputs_embeds=generated)\n                logits = outputs.logits\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                                                    ..., :-1\n                                                    ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n                #\n                top_k = 2000 \n                top_p = 0.98\n                #print(logits)\n                #next_token = transformers.top_k_top_p_filtering(logits.to(torch.int64).unsqueeze(0), top_k=top_k, top_p=top_p)\n                next_token = torch.argmax(logits, -1).unsqueeze(0)\n                next_token_embed = model.gpt.transformer.wte(next_token)\n\n                if tokens is None:\n                    tokens = next_token\n                else:\n                    tokens = torch.cat((tokens, next_token), dim=1)\n                generated = torch.cat((generated, next_token_embed), dim=1)\n               \n                if stop_token_index == next_token.item():\n                    break\n\n            output_list = list(tokens.squeeze().cpu().numpy())\n            \n            output_text = tokenizer.decode(output_list)\n            output_text = filter_ngrams(output_text)\n            generated_list.append(output_text)\n\n    return generated_list[0]\n#from tqdm import tqdm, trange\n\n\ndef _to_caption(pil_image,prompt=''):\n    device = 'cuda:0'\n    image = preprocess(pil_image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        \n        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n        if prompt:\n            generated_text_prefix = generate2(model, tokenizer, prompt=prompt, embed=prefix_embed)\n        else:\n            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n    return generated_text_prefix.replace('\\n',' ').replace('\\xa0','')\n\n","metadata":{"id":"Cl1Kh03qhq3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"qlhoXxtbALdK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ekZVTiP0IC9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n\nimport datetime\n\nimport time\nimport numpy as np\nfrom PIL import Image\nimport json\nfrom torchvision.transforms import functional as TF\n\n\nimport pytz\nimport argparse\n\n\n\nfrom telebot import types\nimport tg_logger\nimport logging\nimport telebot \n\n\n\n\n\nimport json\n\n\n\nimport numpy as np\n\ndef unique_list(l):\n    ulist = []\n    [ulist.append(x) for x in l if x not in ulist]\n    return ulist\n\n\nboot_time = time.time()\nboot_date = datetime.datetime.now(tz=pytz.timezone(\"Europe/Moscow\"))\n\n# ------------- flask config -------------\n\n\n# ------------- bot config -------------\nWEBHOOK_TOKEN = 'aa'\nBOT_TOKEN = '5676745030:AAEcXUG-wF-IMBTbDFl11ZhXwlkVKqOvaMM'\nbot = telebot.TeleBot(BOT_TOKEN)\n\n# ------------- log ---------------\nusers = ['241154130']\n\nalpha_logger = logging.getLogger()\nalpha_logger.setLevel(logging.INFO)\ntg_logger.setup(alpha_logger, token=\"1227347441:AAEnih283opCWcQLFcbghBXc_t1tIp64QXA\", users=users)\n\nlogger = logging.getLogger(\"tg-bot-tti\")\n\n\n\n\nimport re\n\nfrom PIL import Image\n\n\n\n\n\n\n@bot.message_handler(commands=['help', 'start'])\ndef say_welcome(message):\n    '''Displaying the bot's start interface'''\n\n    logger.info(f'</code>@{message.from_user.username}<code> ({message.chat.id}) used /start or /help')\n    bot.send_message(message.chat.id,\n                     \"\"\" Text2Image  generate faces here \"\"\",\n                     parse_mode='html')\n\n#from PIL import Image\n#@bot.message_handler(content_types=['video'])\n#def get_file(message):\n#    file_name = message.json['video']['file_name']\n#    file_info = bot.get_file(message.video.file_id)\n#    with open(file_name, \"wb\") as f:\n#        file_content = bot.download_file(file_info.file_path)\n#        f.write(file_content)\n#   bot.reply_to(message, f\"OK. Сохранил {file_name}\")\n\n\n@bot.message_handler(content_types=['video'])\ndef photo(message):\n    q = message.caption\n    \n    logger.info(f'{message.from_user.username} {q}')\n    file_name = message.json['video']['file_name']\n    file_info = bot.get_file(message.video.file_id)\n    with open(file_name, \"wb\") as f:\n        file_content = bot.download_file(file_info.file_path)\n\n        f.write(file_content)\n    try:\n        path = file_name\n\n\n        video = read_video(path = path, transform = None,frames_num=4)\n        i = image_grid (video,2,2)\n        ans = _to_caption(i, prompt=f'Q:{q} A:')\n\n        #image = PIL.Image.open(\"image.jpg\")\n        #ans = _to_caption(image).replace('<|endoftext|>','')\n        #ans = ' '.join(unique_list(ans.split()))\n        #print(f'{message.from_user.username} {ans}')\n        logger.info(f'{message.from_user.username} {ans}')\n        #bot.send_message(message.chat.id,ans)\n        #class_ = clf(image)[0]\n        bot.send_message(message.chat.id, f'{ans.split(\"A:\")[1]}' )\n    except Exception as e:\n         bot.send_message(message.chat.id, e )\n\n        \n        \n      \n    \n\nif __name__ == '__main__':\n\n  \n  bot.polling(none_stop=True)","metadata":{"id":"wV7qGc8ODxEF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ijbsQSPJFINb"},"execution_count":null,"outputs":[]}]}